{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aefdbbd4-5130-455c-bcef-7c066bc3395a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, when, mean\n",
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc11c41-72d0-4506-9233-fb4aacc22081",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/21 18:32:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/21 18:32:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/12/21 18:32:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder.appName(\"group08\").master(\"spark://<SPARK_IP>:7077\")\n",
    "         .config(\"spark.jars\", \"/opt/spark/jars/gcs-connector-latest-hadoop2.jar\")\n",
    "         .config(\"spark.executor.memory\", \"1G\")  #excutor excute only 2G\n",
    "        .config(\"spark.driver.memory\",\"4G\") \n",
    "        .config(\"spark.debug.maxToStringFields\", \"1000000\") \n",
    "        .config(\"spark.executor.cores\",\"1\") #Cluster use only 3 cores to excute as it has 3 server\n",
    "        .config(\"spark.python.worker.memory\",\"1G\") # each worker use 1G to excute\n",
    "        .config(\"spark.driver.maxResultSize\",\"3G\") #Maximum size of result is 3G\n",
    "        .config(\"spark.kryoserializer.buffer.max\",\"1024M\")\n",
    "         .config(\"spark.port.maxRetries\", \"100\")\n",
    "         .getOrCreate())\n",
    "#config the credential to identify the google cloud hadoop file \n",
    "spark.conf.set(\"google.cloud.auth.service.account.json.keyfile\",\"path_to_CREDENTIAL_FILE\")\n",
    "spark._jsc.hadoopConfiguration().set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem')\n",
    "spark._jsc.hadoopConfiguration().set('fs.gs.auth.service.account.enable', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d691738a-7af2-4401-9b11-241318c86715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_cleansing(i):\n",
    "    kols_path=f\"path_to_kols_table.csv\"\n",
    "    tweets_path=f\"path_to_tweets_table.csv\"\n",
    "    \n",
    "    kols_df = spark.read.csv(kols_path, header=True, inferSchema=True)\n",
    "    tweets_df = spark.read.csv(tweets_path, header=True, inferSchema=True, quote='\"',escape='\"',multiLine=True)\n",
    "\n",
    "    \n",
    "    non_empty_columns = [c for c in kols_df.columns if kols_df.filter(kols_df[c].isNotNull()).count() > 0]\n",
    "    kols_df = kols_df.select(*non_empty_columns)\n",
    "    selected_columns = kols_df.columns[6:13]  \n",
    "    for column in selected_columns:\n",
    "        mean_value = kols_df.select(mean(col(column))).collect()[0][0]\n",
    "        kols_df = kols_df.withColumn(column, when(col(column).isNull(), mean_value).otherwise(col(column)))\n",
    "        kols_df = kols_df.withColumn(column, col(column).cast(IntegerType()))\n",
    "        kols_df = kols_df.withColumn(\"is_verified_num\", when(col(\"is_verified\"), 1).otherwise(0))\n",
    "    combined_df = kols_df.join(tweets_df, kols_df.user_id == tweets_df.author_id)\n",
    "    combined_df = combined_df.dropDuplicates()\n",
    "    df = combined_df.select(\"user_id\", \"tweet_body\")\n",
    "    final_df = df.withColumn(\"tweet_body\", regexp_replace(\"tweet_body\", \"[^A-Za-z0-9\\s]+\", \"\"))\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff113f32-adde-4cad-9434-347380139a5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|            user_id|          tweet_body|\n",
      "+-------------------+--------------------+\n",
      "|1448333842427772933| TekioNFT WL Give...|\n",
      "|1598261796997156864|15\\n\\nI made a po...|\n",
      "|         1053471548|25 Giveaway  5 Ho...|\n",
      "|         1053471548|50 Giveaway  24 H...|\n",
      "|1596825240235032576|Weve created a ke...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saved_path = f\"path_to_save_data\"\n",
    "data_id = [1]\n",
    "for i in data_id:\n",
    "    df = data_cleansing(i)\n",
    "    # df.write.mode(\"append\").option(\"header\", \"true\").csv(saved_path)\n",
    "    df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24baf697-b1d4-43b2-a3aa-320c18b29df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab429e0e-6a24-4f0c-9ba6-d26bbaa27aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
